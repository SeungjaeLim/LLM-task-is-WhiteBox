# LLM-task-is-WhiteBox
Change the scheduler for iteration-level LLM inference to a priority-based scheduler based on the length of the prompt to generate. 
